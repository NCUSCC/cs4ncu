#!/usr/bin/env python3
"""
å†…å®¹å®Œæˆæƒ…å†µåˆ†æè„šæœ¬ (v2.1)

ä» mkdocs.yml å’Œ docs ç›®å½•åˆ†ææ¯ä¸ªå°èŠ‚çš„å®Œæˆæƒ…å†µï¼Œ
ç»Ÿè®¡å­—æ•°å¹¶åšç®€å•åˆ¤æ–­ï¼Œè¾“å‡º CSV å’Œ Markdown æ ¼å¼çš„æŠ¥å‘Šã€‚

ç‰¹æ€§:
- ä» pyproject.toml è¯»å–é…ç½®
- å‘½ä»¤è¡Œå‚æ•°å¯è¦†ç›–é…ç½®
- å°† Git commit ä¿¡æ¯æ·»åŠ åˆ°æŠ¥å‘Šä¸­
- Markdown æŠ¥å‘Šé‡‡ç”¨é¡¶æ ¼å¯¹é½æ ¼å¼
"""

import sys
import csv
import re
import yaml
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

# å°è¯•å¯¼å…¥ä¾èµ–ï¼Œå¹¶åœ¨å¤±è´¥æ—¶ç»™å‡ºæ˜ç¡®æç¤º
try:
    import tomllib  # Python 3.11+
except ImportError:
    try:
        import tomli as tomllib
    except ImportError:
        print(
            "é”™è¯¯ï¼šç¼ºå°‘ toml è§£æåº“ã€‚è¯·è¿è¡Œ `uv add tomli` (Python < 3.11) æˆ–å‡çº§ Pythonã€‚",
            file=sys.stderr,
        )
        sys.exit(1)

try:
    import git
except ImportError:
    print("é”™è¯¯ï¼šç¼ºå°‘ GitPython åº“ã€‚è¯·è¿è¡Œ `uv add gitpython`ã€‚", file=sys.stderr)
    sys.exit(1)

# --- è¾…åŠ©å‡½æ•° ---


def load_config(repo_path: Path, args: argparse.Namespace) -> Dict[str, Any]:
    """åŠ è½½é…ç½®ï¼Œä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > pyproject.toml > é»˜è®¤å€¼"""
    # 1. é»˜è®¤å€¼
    defaults = {
        "md_output": "docs/reports/COMPLETION_REPORT.md",
        "csv_output": "reports/completion_report.csv",
        "min_chars": 50,
        "good_chars": 500,
    }

    # 2. ä» pyproject.toml åŠ è½½
    config = defaults.copy()
    pyproject_path = repo_path / "pyproject.toml"
    if pyproject_path.exists():
        with open(pyproject_path, "rb") as f:
            pyproject_data = tomllib.load(f)
            tool_config = pyproject_data.get("tool", {}).get("completion-analyzer", {})
            config.update(tool_config)

    # 3. å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    cli_args = {key: value for key, value in vars(args).items() if value is not None}
    config.update(cli_args)

    return config


def get_commit_info(repo_path: Path) -> Dict[str, str]:
    """è·å– Git ä»“åº“çš„ commit ä¿¡æ¯"""
    try:
        repo = git.Repo(repo_path, search_parent_directories=True)
        commit = repo.head.commit
        short_hash = commit.hexsha[:7]

        remote_url, commit_link = "", ""
        try:
            remote_url = repo.remotes.origin.url
            if remote_url.endswith(".git"):
                remote_url = remote_url[:-4]
            remote_url = re.sub(r"git@([^:]+):", r"https://\1/", remote_url)
            commit_link = f"{remote_url}/commit/{commit.hexsha}"
        except Exception:
            pass

        return {
            "hash": commit.hexsha,
            "short_hash": short_hash,
            "message": commit.message.strip().split("\n")[0],
            "author": commit.author.name,
            "date": commit.committed_datetime.strftime("%Y-%m-%d %H:%M:%S"),
            "link": commit_link,
        }
    except git.InvalidGitRepositoryError:
        return {"error": "ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ Git ä»“åº“"}
    except Exception as e:
        return {"error": f"è·å– Git ä¿¡æ¯æ—¶å‡ºé”™ï¼š{e}"}


# --- ä¸»åˆ†æç±» ---


class CompletionAnalyzer:
    def __init__(self, config: Dict[str, Any]):
        self.repo_path = Path(config["repo_path"]).resolve()
        self.mkdocs_path = self.repo_path / "mkdocs.yml"
        self.docs_path = self.repo_path / "docs"
        self.skills_index_path = self.docs_path / "skills" / "index.md"

        self.min_chars_threshold = int(config["min_chars"])
        self.good_chars_threshold = int(config["good_chars"])

    def load_mkdocs_config(self) -> Dict:
        try:
            with open(self.mkdocs_path, "r", encoding="utf-8") as f:
                content = f.read()
                content = re.sub(r"!!python/name:[^\s\n]+", '""', content)
                return yaml.safe_load(content)
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½æˆ–è§£æ mkdocs.yml: {e}", file=sys.stderr)
            return {}

    def parse_nav_tree(self, nav_structure: List) -> List[Dict[str, Any]]:
        tree = []

        def _parse_recursive(item: Any):
            if isinstance(item, dict):
                for title, content in item.items():
                    if isinstance(content, str):
                        return {"type": "file", "title": title, "path": content}
                    elif isinstance(content, list):
                        children = [_parse_recursive(sub_item) for sub_item in content]
                        return {
                            "type": "section",
                            "title": title,
                            "children": [c for c in children if c],
                        }
            elif isinstance(item, str):
                title = Path(item).stem.replace("-", " ").replace("_", " ").title()
                return {"type": "file", "title": title, "path": item}
            return None

        if not isinstance(nav_structure, list):
            return []
        for nav_item in nav_structure:
            if node := _parse_recursive(nav_item):
                tree.append(node)
        return tree

    def parse_todo_checklist(self) -> Dict[str, bool]:
        todo_status = {}
        if not self.skills_index_path.exists():
            return todo_status
        try:
            content = self.skills_index_path.read_text(encoding="utf-8")
            pattern = (
                r'- \[([ x])\]\s*(?:\*\*)?[`"]?([^`"]*?)[`"]?(?:\*\*)?\s*\(`([^)]+)`\)'
            )
            matches = re.findall(pattern, content, re.MULTILINE)
            for is_done, _, file_path in matches:
                todo_status[file_path] = is_done.lower() == "x"
        except Exception as e:
            print(f"è­¦å‘Šï¼šè§£æ TODO æ¸…å•æ—¶å‡ºé”™ï¼š{e}", file=sys.stderr)
        return todo_status

    def analyze_file(self, file_path: str) -> Dict[str, Any]:
        full_path = self.docs_path / file_path
        result = {
            "file_path": file_path,
            "exists": False,
            "char_count": 0,
            "word_count": 0,
            "line_count": 0,
            "status": "missing",
            "completion_level": "none",
        }
        if full_path.is_file():
            try:
                content = full_path.read_text(encoding="utf-8")
                result.update(
                    {
                        "exists": True,
                        "char_count": len(content),
                        "word_count": len(re.findall(r"\w+", content)),
                        "line_count": len(content.splitlines()),
                    }
                )
                if result["char_count"] == 0:
                    result.update({"status": "empty", "completion_level": "none"})
                elif result["char_count"] < self.min_chars_threshold:
                    result.update({"status": "minimal", "completion_level": "low"})
                elif result["char_count"] < self.good_chars_threshold:
                    result.update({"status": "basic", "completion_level": "medium"})
                else:
                    result.update({"status": "substantial", "completion_level": "high"})
            except Exception as e:
                print(f"é”™è¯¯ï¼šè¯»å–æ–‡ä»¶ {file_path} å¤±è´¥ï¼š{e}", file=sys.stderr)
                result["status"] = "error"
        return result

    def generate_report_tree(self) -> List[Dict[str, Any]]:
        config = self.load_mkdocs_config()
        nav_structure = config.get("nav", [])
        if not nav_structure:
            print("é”™è¯¯ï¼šmkdocs.yml ä¸­æœªæ‰¾åˆ° 'nav' é…ç½®ã€‚", file=sys.stderr)
            return []
        report_tree = self.parse_nav_tree(nav_structure)
        todo_status = self.parse_todo_checklist()
        processed_files = set()

        def _analyze_node(node: Dict[str, Any]):
            if node["type"] == "file":
                if (file_path := node["path"]) in processed_files:
                    return
                processed_files.add(file_path)
                analysis = self.analyze_file(file_path)
                node.update(analysis)
                node["category"] = self.categorize_file(file_path)
                node["manual_status"] = todo_status.get(file_path)
                if node["manual_status"] is True:
                    node["final_status"] = "completed"
                elif node["manual_status"] is False:
                    node["final_status"] = "todo"
                else:
                    node["final_status"] = node["status"]
                node["size_assessment"] = self.assess_content_size(node["char_count"])
            elif node["type"] == "section" and "children" in node:
                for child in node["children"]:
                    _analyze_node(child)

        for root_node in report_tree:
            _analyze_node(root_node)
        return report_tree

    def _flatten_report_tree(self, report_tree: List[Dict]) -> List[Dict]:
        flat_list = []

        def _traverse(node):
            if node["type"] == "file":
                flat_list.append(node)
            elif node["type"] == "section" and "children" in node:
                for child in node["children"]:
                    _traverse(child)

        for node in report_tree:
            _traverse(node)
        return flat_list

    def categorize_file(self, file_path: str) -> str:
        parts = Path(file_path).parts
        if not parts:
            return "å…¶ä»–"
        top_level, sub_dir = parts[0], parts[1] if len(parts) > 1 else None
        if top_level == "skills" and sub_dir:
            return {
                "mindset": "è®¤çŸ¥ä¸å¿ƒæ™º",
                "learning": "å­¦ä¼šå­¦ä¹ ",
                "communication": "æ²Ÿé€šä¸åä½œ",
                "tools": "å·¥å…·ä¸ç³»ç»Ÿ",
                "growth": "æˆé•¿ä¸è§„åˆ’",
            }.get(sub_dir, "ç¬¬é›¶ç‚¹äº”è¯¾å ‚")
        return {
            "course": "æ ¸å¿ƒè¯¾ç¨‹",
            "skills": "ç¬¬é›¶ç‚¹äº”è¯¾å ‚",
            "ncu": "æ˜Œå¤§ä¸“å±",
            "guides": "é¡¹ç›®å…±å»º",
            "before": "å¼€å§‹ä¹‹å‰",
            "aboutus": "å…³äºæˆ‘ä»¬",
        }.get(top_level, "å…¶ä»–")

    def assess_content_size(self, char_count: int) -> str:
        if char_count == 0:
            return "ç©ºç™½"
        if char_count < 50:
            return "æå°‘"
        if char_count < 200:
            return "è¾ƒå°‘"
        if char_count < 500:
            return "é€‚ä¸­"
        if char_count < 1000:
            return "è¾ƒå¤š"
        if char_count < 2000:
            return "ä¸°å¯Œ"
        return "éå¸¸ä¸°å¯Œ"

    def save_to_csv(self, report_list: List[Dict[str, Any]], output_path_str: str):
        output_path = self.repo_path / output_path_str
        output_path.parent.mkdir(parents=True, exist_ok=True)
        fieldnames = [
            "title",
            "file_path",
            "category",
            "exists",
            "char_count",
            "word_count",
            "line_count",
            "status",
            "completion_level",
            "manual_status",
            "final_status",
            "size_assessment",
        ]
        with open(output_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
            writer.writeheader()
            writer.writerows(report_list)
        print(f"âœ… CSV æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{output_path.relative_to(self.repo_path)}")

    def save_to_markdown(
        self, report_tree: List[Dict], output_path_str: str, commit_info: Dict
    ):
        """å°†æŠ¥å‘Šæ ‘ä¿å­˜ä¸ºç¾è§‚çš„ã€é¡¶æ ¼å¯¹é½çš„ Markdown æ–‡ä»¶"""
        output_path = self.repo_path / output_path_str
        output_path.parent.mkdir(parents=True, exist_ok=True)

        header = [
            f"# å†…å®¹å®Œæˆæƒ…å†µæŠ¥å‘Š",
            f"> æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        ]
        if "error" not in commit_info:
            commit_link_md = (
                f"[`{commit_info['short_hash']}`]({commit_info['link']})"
                if commit_info.get("link")
                else f"`{commit_info['short_hash']}`"
            )
            header.extend(
                [
                    "## æŠ¥å‘Šä¸Šä¸‹æ–‡",
                    f"- **ç‰ˆæœ¬æ¥æº**: {commit_link_md} - *{commit_info['message']}*",
                    f"- **ç”Ÿæˆè„šæœ¬**: `{Path(sys.argv[0]).name}`\n",
                ]
            )
        else:
            header.append(f"_{commit_info['error']}_\n")

        header.extend(
            [
                "## å®Œæˆåº¦æ¦‚è§ˆ",
                "- `[ ]` è¡¨ç¤ºå¾…åŠæˆ–å†…å®¹ä¸è¶³",
                "- `[x]` è¡¨ç¤ºå·²å®Œæˆæˆ–å†…å®¹å……å®\n",
            ]
        )
        lines = header

        def _render_node_md(node: Dict, level: int):
            # --- ä¿®æ”¹ç‚¹åœ¨è¿™é‡Œ ---
            if node["type"] == "file":
                is_complete = (
                    node.get("final_status") == "completed"
                    or node.get("status") == "substantial"
                )
                checkbox = "[x]" if is_complete else "[ ]"
                details = (
                    f"{node.get('char_count', 0)}å­—ï¼Œ{node.get('final_status', 'æœªçŸ¥')}"
                )
                # ç§»é™¤ç¼©è¿›
                lines.append(
                    f"- {checkbox} **{node.get('title', 'æ— æ ‡é¢˜')}** (`{node.get('path', '')}`) - *{details}*"
                )

            elif node["type"] == "section":
                files = self._flatten_report_tree([node])
                total = len(files)
                completed = (
                    sum(
                        1
                        for f in files
                        if f.get("final_status") == "completed"
                        or f.get("status") == "substantial"
                    )
                    if total > 0
                    else 0
                )
                progress = f"({completed}/{total})" if total > 0 else ""
                # ç§»é™¤ç¼©è¿›ï¼Œå¹¶ä½¿ç”¨æ ‡é¢˜çº§åˆ«ä½“ç°å±‚æ¬¡
                heading_level = min(level + 2, 6)  # ä»##å¼€å§‹ï¼Œæœ€æ·±åˆ°######
                lines.append(
                    f"\n{'#' * heading_level} {node.get('title', 'æœªå‘½åç« èŠ‚')} {progress}"
                )
                if "children" in node:
                    for child in node["children"]:
                        # é€’å½’è°ƒç”¨ï¼Œå±‚çº§ +1
                        _render_node_md(child, level + 1)

        for root_node in report_tree:
            # åˆå§‹å±‚çº§ä¸º 0
            _render_node_md(root_node, 0)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        print(f"âœ… Markdown æŠ¥å‘Šå·²ä¿å­˜åˆ°ï¼š{output_path.relative_to(self.repo_path)}")

    def print_summary(self, report_list: List[Dict[str, Any]]):
        total_files = len(report_list)
        if total_files == 0:
            print("\n=== å®Œæˆæƒ…å†µæ±‡æ€» ===")
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶è¿›è¡Œåˆ†æã€‚è¯·æ£€æŸ¥ mkdocs.yml çš„ 'nav' é…ç½®ã€‚")
            return

        existing_files = sum(1 for r in report_list if r.get("exists"))
        substantial_files = sum(
            1 for r in report_list if r.get("status") == "substantial"
        )

        print("\n=== ğŸ“Š å®Œæˆæƒ…å†µæ±‡æ€» ===")
        print(f"æ€»æ–‡ä»¶æ•° (æ¥è‡ª nav): {total_files}")
        print(f"å·²å­˜åœ¨æ–‡ä»¶ï¼š{existing_files} ({existing_files/total_files*100:.1f}%)")
        print(
            f"å†…å®¹å……å®çš„æ–‡ä»¶ (>{self.good_chars_threshold}å­—): {substantial_files} ({substantial_files/total_files*100:.1f}%)"
        )


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æ MkDocs é¡¹ç›®çš„å†…å®¹å®Œæˆæƒ…å†µï¼Œå¹¶ç”ŸæˆæŠ¥å‘Šã€‚"
    )
    parser.add_argument(
        "--repo-path", type=str, default=".", help="ä»“åº“æ ¹ç›®å½•è·¯å¾„ (é»˜è®¤ï¼šå½“å‰å·¥ä½œç›®å½•)"
    )
    parser.add_argument(
        "--md-output", type=str, help="Markdown æŠ¥å‘Šçš„è¾“å‡ºè·¯å¾„ (è¦†ç›– pyproject.toml)"
    )
    parser.add_argument(
        "--csv-output", type=str, help="CSV æŠ¥å‘Šçš„è¾“å‡ºè·¯å¾„ (è¦†ç›– pyproject.toml)"
    )
    parser.add_argument(
        "--min-chars", type=int, help="â€œå†…å®¹æå°‘â€çš„å­—ç¬¦æ•°é˜ˆå€¼ (è¦†ç›–é…ç½®)"
    )
    parser.add_argument(
        "--good-chars", type=int, help="â€œå†…å®¹å……å®â€çš„å­—ç¬¦æ•°é˜ˆå€¼ (è¦†ç›–é…ç½®)"
    )
    parser.add_argument(
        "--quiet", action="store_true", help="é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºæŠ¥å‘Šç”Ÿæˆä¿¡æ¯"
    )

    args = parser.parse_args()

    repo_path = Path(args.repo_path).resolve()
    config = load_config(repo_path, args)
    config["repo_path"] = repo_path

    analyzer = CompletionAnalyzer(config)

    if not args.quiet:
        print(f"ğŸš€ æ­£åœ¨åˆ†æé¡¹ç›®ï¼š{repo_path.name}")

    report_tree = analyzer.generate_report_tree()

    if not report_tree:
        print("åˆ†æä¸­æ­¢ï¼Œæœªç”Ÿæˆä»»ä½•æŠ¥å‘Šã€‚", file=sys.stderr)
        return

    report_list = analyzer._flatten_report_tree(report_tree)
    commit_info = get_commit_info(repo_path)

    analyzer.save_to_markdown(report_tree, config["md_output"], commit_info)
    analyzer.save_to_csv(report_list, config["csv_output"])

    if not args.quiet:
        analyzer.print_summary(report_list)
    else:
        print("æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ã€‚")


if __name__ == "__main__":
    main()
